{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from matplotlib.pyplot import imshow\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.nn.init as init\n",
    "import torch.utils.model_zoo as model_zoo\n",
    "from torchvision import models\n",
    "import torch.multiprocessing as mp\n",
    "from torchvision import transforms\n",
    "\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import math\n",
    "import time\n",
    "import importlib\n",
    "import os\n",
    "import argparse\n",
    "import copy\n",
    "import datetime\n",
    "import random\n",
    "import sys\n",
    "import json\n",
    "\n",
    "### My libs\n",
    "from core.utils import Stack, ToTorchFormatTensor\n",
    "from core.utils import ZipReader\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ref_length = 10\n",
    "neighbor_stride = 5\n",
    "default_fps = 15\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(1)\n",
    "device = 'cuda'\n",
    "    \n",
    "MASK_TYPE = 'object'\n",
    "CKPT = 'checkpoints/sttn.pth'\n",
    "DATA_NAME = 'davis'\n",
    "MODEL_NAME = 'vis'\n",
    "w, h = 432, 240\n",
    "\n",
    "_to_tensors = transforms.Compose([\n",
    "  Stack(),\n",
    "  ToTorchFormatTensor()])\n",
    "\n",
    "\n",
    "def get_ref_index(neighbor_ids, length, split):\n",
    "  ref_index = []\n",
    "  for i in range(0, length, ref_length):\n",
    "    if not i in neighbor_ids:\n",
    "      ref_index.append(i)\n",
    "  return ref_index\n",
    "\n",
    "\n",
    "def get_mask(vname, f):\n",
    "  if MASK_TYPE == 'fixed':\n",
    "    m = np.zeros((h, w), np.uint8)\n",
    "    m[h//2-h//8:h//2+h//8, w//2-w//8:w//2+w//8] = 255\n",
    "    return Image.fromarray(m)\n",
    "  elif MASK_TYPE == 'object':\n",
    "    mname = f\"{str(f).zfill(5)}.png\"\n",
    "    m = ZipReader.imread('datasets/{}/Annotations/{}.zip'.format(DATA_NAME, vname), mname).convert('L')\n",
    "    m = np.array(m)\n",
    "    m = np.array(m>0).astype(np.uint8)\n",
    "    m = cv2.resize(m, (w,h), cv2.INTER_NEAREST)\n",
    "    m = cv2.dilate(m, cv2.getStructuringElement(cv2.MORPH_CROSS,(3,3)), iterations=4)\n",
    "    return Image.fromarray(m*255)\n",
    "  elif MASK_TYPE == 'random_obj':\n",
    "    m = ZipReader.imread('datasets/random_masks/{}.zip'.format(DATA_NAME),\\\n",
    "      '{}.png'.format(vname)).resize((w, h))\n",
    "    m = np.array(m)\n",
    "    m = np.array(m>0).astype(np.uint8)\n",
    "    return Image.fromarray(m*255)\n",
    "  else:\n",
    "    raise NotImplementedError(f\"Mask type {MASK_TYPE} not exists\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# save ann to img by pyplt\n",
    "def save_by_pyplt(I, anns, fname, cmap):\n",
    "    dpi=100\n",
    "    shape=np.shape(I)[0:2][::-1]\n",
    "    size = [float(i)/dpi for i in shape]\n",
    "    fig = plt.figure()\n",
    "    fig.set_size_inches(size)\n",
    "    ax = plt.Axes(fig,[0,0,1,1])\n",
    "    ax.set_axis_off()\n",
    "    fig.add_axes(ax)\n",
    "    ax.imshow(I)\n",
    "    if anns is not None:\n",
    "      ax.imshow(anns, alpha=0.7, cmap=cmap)\n",
    "#     fig.savefig(fname, dpi=dpi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and version\n",
    "net = importlib.import_module('model.' + MODEL_NAME)\n",
    "model = net.InpaintGenerator().to(device)\n",
    "data = torch.load(CKPT, map_location = device) \n",
    "model.load_state_dict(data['netG'])\n",
    "print('loading from: {}'.format(CKPT))\n",
    "model.eval()\n",
    "\n",
    "# prepare dataset\n",
    "save_path = \"./\"\n",
    "with open('datasets/{}/test.json'.format(DATA_NAME), 'r') as f:\n",
    "    video_dict = json.load(f)\n",
    "    video_names = list(videos_dict.keys())\n",
    "video_names.sort()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vi = np.random.randint(0, len(video_names))\n",
    "vname = video_names[vi]\n",
    "print(vi)\n",
    "\n",
    "frame_names = [f\"{str(i).zfill(5)}.jpg\" for i in range(video_dict[vname])]\n",
    "num_frames = video_dict[vname]\n",
    "print('{} of {} frames ...'.format(vname, num_frames))\n",
    "masks = []\n",
    "frames = []\n",
    "orig_frames = []\n",
    "# preprocess data\n",
    "for f, fname in enumerate(frame_names):\n",
    "  img = ZipReader.imread('datasets/{}/JPEGImages/{}.zip'.format(DATA_NAME, vname), fname).convert('RGB')\n",
    "  orig_frames.append(img)\n",
    "  frames.append(img.resize((w, h)))\n",
    "  m = get_mask(vname, f)\n",
    "  masks.append(m)\n",
    "binary_masks = [np.expand_dims((np.array(i)!=0).astype(np.uint8), 2) for i in masks]\n",
    "comp_frames = [None]*len(frame_names)\n",
    "pred_frames = [None]*len(frame_names)\n",
    "\n",
    "feats = _to_tensors(frames).unsqueeze(0)*2-1\n",
    "frames = [np.array(i).astype(np.uint8) for i in frames]\n",
    "masks =  _to_tensors(masks).unsqueeze(0)\n",
    "feats, masks = feats.to(device), masks.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "with torch.no_grad():\n",
    "  feats = model.encoder((feats*(1-masks).float()).view(num_frames, 3, h, w))\n",
    "  _, c, feat_h, feat_w = feats.size()\n",
    "  feats = feats.view(1, num_frames, c, feat_h, feat_w)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# begin inference \n",
    "for f in range(len(frame_names)//2, len(frame_names), neighbor_stride):\n",
    "  neighbor_ids = [i for i in range(max(0,f-neighbor_stride), min(len(frame_names), f+neighbor_stride+1))]\n",
    "  ref_ids = get_ref_index(neighbor_ids, len(frame_names), ref_length)\n",
    "  with torch.no_grad():\n",
    "    current_feat, attn, mm = model.infer(feats[0,neighbor_ids+ref_ids, :,:,:], masks[0,neighbor_ids+ref_ids,:,:,:])\n",
    "    current_img = torch.tanh(model.decoder(current_feat[:len(neighbor_ids),:,:,:])).detach()\n",
    "#     current_img = torch.tanh(model.decoder(current_feat)).detach()\n",
    "    pred_img = (current_img+1)/2\n",
    "    pred_img = pred_img.cpu().permute(0,2,3,1).numpy()*255\n",
    "    # visualize attention \n",
    "    vis_img = np.array(pred_img[0]).astype(np.uint8)\n",
    "    imshow(vis_img)\n",
    "    \n",
    "  print('show...')\n",
    "  break\n",
    "  input('Enter something...')\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('neighbor_ids: ', neighbor_ids, len(neighbor_ids))\n",
    "print('ref ids: ', ref_ids, len(ref_ids))\n",
    "ids = neighbor_ids+ref_ids\n",
    "print('ids: ', ids, len(ids))\n",
    "\n",
    "img = np.array(pred_img[len(neighbor_ids)//2+1]).astype(np.uint8)\n",
    "input = frames[f]*(1-binary_masks[f])+255*binary_masks[f]\n",
    "w, h = 432, 240\n",
    "a = mm[len(neighbor_ids)//2+1].cpu().numpy().astype(np.float32)\n",
    "print('selected attention patch size: ', mm.size()[0])\n",
    "\n",
    "# select a target patch \n",
    "start = np.argmax(a)\n",
    "end  = a.size - np.argmax(a[::-1]) - 1\n",
    "a = cv2.resize(a, (w, h))\n",
    "selected_patch=(start+end)//2\n",
    "\n",
    "print(start, end, selected_patch)\n",
    "print('target frame as {}'.format(f))\n",
    "save_by_pyplt(frames[f], None, 'groundtruth_{}.jpg'.format(str(f).zfill(3)), 'Reds')\n",
    "save_by_pyplt(input, None, 'input_{}.jpg'.format(str(f).zfill(3)), 'Reds')\n",
    "save_by_pyplt(img, None, 'output_{}.jpg'.format(str(f).zfill(3)), 'Reds')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "save_by_pyplt(input, None, 'target.jpg', 'Reds')\n",
    "plt.show()\n",
    "\n",
    "# find all attention value for selected patch \n",
    "np_attn = attn.cpu().numpy()\n",
    "val = np_attn[selected_patch]\n",
    "\n",
    "# sort attention \n",
    "ind = np.unravel_index(np.argsort(val, axis=None), val.shape)\n",
    "ind_t, ind_x, ind_y = list(ind[0]), list(ind[1]), list(ind[2])\n",
    "ind_t.reverse()\n",
    "seen = set()\n",
    "ind_t = np.array([x for x in ind_t if x not in seen and not seen.add(x)])\n",
    "print(ind_t)\n",
    "\n",
    "# show top 5 attended frames\n",
    "for t in ind_t[:5]:\n",
    "  print('Attention from frame: {}'.format(ids[t]))\n",
    "  ref = frames[ids[t]]*(1-binary_masks[ids[t]])+255*binary_masks[ids[t]]\n",
    "  a = cv2.resize(val[t], (w, h))\n",
    "  save_by_pyplt(ref, a, 'ref_{}.jpg'.format(str(ids[t]).zfill(3)), None)\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
